####v3:没有旋转,默认的adam参数,其他和论文一致
####v4:有旋转,通道变化,默认adam   12w*60epoich
		nme2d 0.02744403375175568 init 0.027443867754818895
		nme3d 0.04497236390491756 init 0.044971630436985946
		landmark2d 0.023305583597160874 init 0.02330539555431677
		landmark3d 0.04249470422370359 init 0.04249401983455507
		gtlandmark2d 0.023334158426916055 init 0.023333931757022898
		gtlandmark3d 0.042540760313416734 init 0.04254005307292635

        nme2d 0.03897261261174028
        nme3d 0.0637040727516813
        landmark2d 0.033023160795215514
        landmark3d 0.060168476723134516
        gtlandmark2d 0.03304077044337981
        gtlandmark3d 0.06021612284874482



####prnv5:没有高斯噪声  24w*30epoch
        nme2d 0.03861459784020665
        nme3d 0.06184378041573199
        landmark2d 0.034380764290690424
        landmark3d 0.058864043913781645
        gtlandmark2d 0.0334626781860397
        gtlandmark3d 0.05831176808577313


####论文:
    [[	nmd:0.02679545 0.04604859
	ld:0.02296228 0.04386363
	gtld0.02283289 0.04386107]]

    (fix bug of bbox length)
        0.03805276
        0.06568615
        0.03271805
        0.06251525
        0.03253049
        0.06251704
        
     (11.6)
     [0.03102195 
     0.04333708 
     0.03241197 
     0.04660079 
     0.03256566 
     0.04685825
        icp:0.04515736 
        icp2:0.02068132]
####aprn_v1:加了一个distination module和正则化的结果(明显性能下降)
        nme2d 0.03965755857578315
        nme3d 0.06321064526769553
        landmark2d 0.03557098467182368
        landmark3d 0.06009170249197632
        gtlandmark2d 0.03548280813925139
        gtlandmark3d 0.06015663169656289


####light_prn
        _v1:论文参数量  需要进一步调参/数据增强等
        nme2d 0.04388941008270951
        nme3d 0.06855528197074574
        landmark2d 0.0387821509311907
        landmark3d 0.06491639921907336
        gtlandmark2d 0.03916383116293735
        gtlandmark3d 0.06526354699573683

        v2:添加了默认的正则项 activation等之前没注意的东西,由于BN的keras实现中参数数量多于tensorflow 所以总参数数量略微多一点
            在训练时完全按照论文的方式数据增强  遮挡部分使用的是random erase
        nme2d 0.039920023259454096
        nme3d 0.0643256840910514
        landmark2d 0.03417898349044844
        landmark3d 0.060291870658285916
        gtlandmark2d 0.0341992616159814
        gtlandmark3d 0.060423312304119674

        v3 多跑了几轮  推测templateLoss到0.0410时能复现论文结果  目前0.0419
        nme2d 0.03946417580606594
        nme3d 0.06384182354340222
        landmark2d 0.033679383514449
        landmark3d 0.05980661947838962
        gtlandmark2d 0.03376040019464511
        gtlandmark3d 0.05997065924317083
####aprn_v2:论文参数数量+CBMA 需要进一步调参/数据增强,相比light_prn略有提升
        nme2d 0.040603508838508044
        nme3d 0.0660032366346785
        landmark2d 0.036365815570577976
        landmark3d 0.06270445635076612
        gtlandmark2d 0.0360549575844688
        gtlandmark3d 0.06257568786263584


####offset_prnv1:
        权重:0.01  1  0.1  0.1 0.1






####init_v2 (2019-10-14):
    nme2d 0.03749504179325621
    nme3d 0.05204750241731822
    landmark2d 0.03285076037608087
    landmark3d 0.04749819666519761
    gtlandmark2d 0.033147984788385494
    gtlandmark3d 0.771248694524152
    
    
####attention2-3.68
    (uv_idx)
    nme2d 0.0363423051034315
    nme3d 0.05116356197977222
    landmark2d 0.0311440198533237
    landmark3d 0.04638390733255073
    gtlandmark2d 0.03144776849629714
    gtlandmark3d 0.7732013536649072
    icp 0.038175174127193824
    
    nme2d 0.029969781220458774
    nme3d 0.04204380483065716
    landmark2d 0.03132885884074494
    landmark3d 0.045349718879908324
    icp 0.044036705694854864
    icp2 0.020144277779362448

####init warmup(11-1)
    (block600)
    nme2d 0.03746904949610367
    nme3d 0.05189499186837182
    landmark2d 0.032808109061326834
    landmark3d 0.047672977124340835
    (block630)
    nme2d 0.0372361505045347
    nme3d 0.0520247718103225
    landmark2d 0.03206665419880301
    landmark3d 0.047274475677870215
    icp2 0.020777389370857303
    
####siam warmup
    (10-31)block600 visible
    nme2d 0.030307061689362744
    nme3d 0.0417609638735415
    landmark2d 0.031880436341743916
    landmark3d 0.04515002635540441
    icp 0.04137272365393564
    icp2 0.01893310731339709
    
    
####11.6
    my baseline:
        nme2d 0.030550409066631947
        nme3d 0.0427010075212377
        landmark2d 0.03206665348308161
        landmark3d 0.046061055537313225
        icp 0.04544273908606006
        icp2 0.020777389273168533
       论文model
            [0.03102195 
             0.04333708 
             0.03241197 
             0.04660079 
             0.03256566 
             0.04685825
                icp:0.04515736 
                icp2:0.02068132]

    init+smoothloss(0.1) 和baseline相比没太大区别(baseline已经超过原论文指标)
        nme2d 0.030531826702546373
        nme3d 0.042255709977254995
        landmark2d 0.03244720254419371
        landmark3d 0.046097466100938615
        icp 0.04500182954593355
        icp2 0.020582081365441825
    
    visible final0.1 offset0.5 kpt1 attention0.1
        nme2d 0.030170221396088735
        nme3d 0.0417460264909389
        landmark2d 0.03191528393281624
        landmark3d 0.04521814687736332
        icp 0.04110096505326153
        icp2 0.018806621996566988
        
        kpt branch:
        landmark2d 0.03254090938018635
        landmark3d 0.04646939398441464
        
        nme2d 0.030107257085583053
        nme3d 0.04143125745187194
        landmark2d 0.03140148746594787
        landmark3d 0.04458198898145929
        icp 0.04091062818071751
        icp2 0.018721347268683144
        
     attention lossrate(0.1)
        nme2d 0.030113361600287263
        nme3d 0.04184347375606234
        landmark2d 0.031806760095525535
        landmark3d 0.045342183772940185
        icp 0.04453457681213955
        icp2 0.020360541638661522
            
    
    given:
    0-30        30-60       60-90       mean        std
    0.0291      0.03474868  0.04947507  0.03777     0.0319248
    
    baseline:
    0.0271      0.03469373  0.048302    0.036725    0.0188298
    [0.02717956 0.02487882 0.03535992] [0.03469373 0.03366667 0.04731255] [0.04830231 0.04871199 0.06580324] [0.0367252  0.03575249 0.0494919 ] 0
    
    visible:
    0.026602846 0.035085093 0.04860075  0.036762897  0.017717158
    [0.02660284 0.0243786  0.03518024] [0.0350851  0.03358473 0.04624126] [0.04860075 0.04808036 0.0626327 ] [0.0367629  0.0353479  0.04801806] 
    
    attention:
    0.026905479 0.034301586 0.04844786  0.03655164   0.020811612
    [0.02690548 0.02470805 0.03549175] [0.03430158 0.0330296  0.04626507] [0.04844786 0.04867535 0.06548646] [0.03655164 0.035471   0.04908109] 
   
    
    
    
    